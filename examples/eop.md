Пример исследовательской идеи, [статья](https://proceedings.mlr.press/v162/kurenkov22a.html) на основе которой была опубликована на ICML 2022. Важно отметить, что в пропозале могут быть пропущены какие-то важные детали по введению в область/проблему (потому что ЦА пропозала шарит за offline-rl), но если вы вдруг пишите про что-то, и не уверены насколько мы погружены в эту проблему -- то лучше расписать подробнее.

## Expected Online Performance (оценка Offline-RL алгоритмов)

### TL;DR

Опишите идею в паре предложений.

- Более полный подход к сравнению Offline-RL алгоритмов, который учитывает перебор гиперпараметров и деплой в онлайн.

### Motivation

Опишите проблему, которую хочется решить. Почему это важно?


- **Немного контекста:** Offline-RL алгоритмы сравнивают между собой на основе лучшего финального скора, i.e., их тренируют 1 миллион шагов, потом вытаскивают последнюю политику и оценивают ее. 
Однако, есть нюанс -- еще перебираются гиперпараметры. В итоге мы делаем онлайн оценку не один раз, а N, где N -- это сколько гиперов перебрали.
Это противоречит presupposed ограничению offline-rl, где мы имеем право оценить политику только один раз.
Более того, наверняка, какие-то алгоритмы более чувствительны к гиперпараметрам, поэтому существующая оценка не совсем честная и не учитывает потенциальные ограничения practitioners.

- **Проблема и мотивация:** Следовательно, встает проблема: если я не могу задеплоить N политик на прод, а только K < N, то какой метод мне нужно выбрать? Существующие подходы к сравнению не дают ответа на этот вопрос.
А это важно, т.к. мы, например, не можем задеплоить 30 политик на прод и сравнить их, но часто можем задеплоить, условные, 2-3. Более того, чаще всего, авторы не указывают сколько гиперпараметров они перебрали, то есть могут сравниваться не особо сравниваемые числа в статьях (например, одни перебрали 100 гиперов, а другие только 2).


### Related Work

Что уже есть по этой теме? В первую очередь статьи, потом имплементации и всё остальное. Напишите по 1-2 предложению для каждой из статьи, чтобы было понятно, о чем они, какие результаты они получили и насколько им можно доверять.

#### Обсуждают, что проблема сравнения Offline-RL алгоритмов существует
- 2021, [Quantile Filtered Imitation Learning](https://arxiv.org/abs/2112.00950)
  - !!! Здесь фиксируют бюджет (N=4) для сравнения алгоритмов, и показывают, что не все так радужно. При этом бюджет не варьируют, оценку не предлагают (а тут мы и ворвемся).
- 2019, [Behavior Regularized Offline Reinforcement Learning](https://arxiv.org/abs/1911.11361)
  - Вообще, статья про новый offline-rl алгос BRAC. Но, активно обсуждают как перебирали гиперпараметры в секции 4.6 и подчеркивают проблему оценки алгосов.
- 2020, [Deployment-Efficient Reinforcement Learning via Model-Based Offline Optimization](https://arxiv.org/abs/2006.03647)
  - Вот здесь задумались о том, что мы хотим получить наилучший перформанс задеплоив как можно меньше политик. Но! Здесь алгоритм дотренировывается на собранных данных. Это немного другой сетап, больше похожий на constrainted-finetuning.
- 2021, [Model-based Uncertainty Regularized and Sample Efficient Batch Optimization for Deployment Constrained
Reinforcement Learning](https://arxiv.org/abs/2102.11448)
  - Same as above, но предлагают новый метод. По этим двум работам есть нюанс, они тоже гиперпараметры перебирают для них)0 Поэтому ортогональные как по мне.

#### Перебор гиперпараметров в Offline-RL
- 2020, [Hyperparameter Selection for Offline Reinforcement Learning](https://arxiv.org/abs/2007.09055)
  - Предлагают методы для выбора гиперпараметров ORL алгоритмов. Говорят, что offline RL algorithms are not robust to hyperparameter choices. Но все равно не учитывают разные бюджеты K, просто фиксируют и забивают. Наверное, самая близкая статья, с ней надо будет аккуратно сравниваться.
- 2021, [Active Offline Policy Selection](https://arxiv.org/abs/2106.10251)
  - Закрывают feedback loop в OPS, т.е. каким-то образом используют получившуюся оценку от задеплоенной политики, чтобы перевзвесить другие и выбрать новую. Работе хорошо, проблема опять же ортогональная.

#### Методы для Offline Policy Selection (i.e., пытаемся выбрать лучшую политику из заданного сета политик)
- 2020, [Offline Policy Selection under Uncertainty](https://arxiv.org/abs/2012.06919)
  - BayesDICE, чо-то геморное, но активно цитируют его. Кажется не напрямую релевантен, но процитировать полезно.
- 2019, [Study of Off-Policy Policy Evaluation for Reinforcement Learning](https://arxiv.org/abs/1911.06854)
  - Вот тут сравнивают очень много разных OPE-методов, выглядит все это дело, честно говоря не очень рабочим. Поэтому, мне кажется, наш подход может быть еще более актуальным в свете того, что OPE методы работают плохо.

#### Методы сравнения алгоритмов из других областей
- 2019, [Show Your Work: Improved Reporting of Experimental Results](https://aclanthology.org/D19-1224/)
  - Предлагают метод Expected Validation Performance (EVP), который позволяет сравнивать алгоритмы с учетом перебора гиперпараметров (то шо нужно). Тут даже есть математика под метод оценки.

### Idea

Как предлагается решить проблему? Что нового в этом решении? Здесь хочется увидеть достаточно подробное описание.

- **Что делаем?** Давайте воспользуемся Expected Validation Performance из области NLP. Она должна применяться почти 1-к-1 в offline-rl, если гиперы семплируются случайным образом, но надо еще математику бы расписать.
- **В чем вижу новизну?** Как мне кажется, новизна будет не в самой технике, а в тех результатах, который наверняка вскроются: (1) что preferences между алгоритмами зависят от бюджета на перебор гиперпараметров, и (2) в чистом offline-rl сетапе (где K=1) результаты будут не такие радужные, как их репортят в статьях.

### Discussion

Почему это будет работать? Может быть, есть работы, из которых можно сделать такой вывод или построить некую интуицию. Bonus: почему это может не заработать? На что стоит обратить внимание.

- **EVP + NLP = Success** Мы знаем, что EVP успешно применяется в области NLP для сравнения моделей между собой, например, насколько дольше их тюнить. Метод мы будем использовать тот же.
- **NLP: Разные бюджеты, разные победители** По опыту NLP мы видим, что одни модели работают лучше других на разных бюджетах, верю, что в offline-rl так же.

### Experiments A

Как быстро проверить идею на работоспособность (условно, за пару недель), чтобы решить, стоит ли копать в нее дальше.

#### Какие датасеты придется использовать? Доступны ли они публично?
- Можно взять [NeoRL](https://arxiv.org/abs/2102.00714) датасеты, они публично доступны.

#### Какие есть baselines для предложенного метода? Есть ли они вообще? Доступны ли они публично?
- Не совсем применимо, т.к. не предлагаем новый алгос. 
- Но т.к. предлагаем новую методологию, то надо сравниться со старой -- maximum performance.

#### Какие метрики будут использоваться, чтобы показать, что идея работает? Насколько сложно посчитать эти метрики
- Просто онлайн оценки из gym-env'ов. На их основе построим EVP графики, посмотрим пересекаются ли они для разных алгоритмов и насколько сильно отличаются на разных бюджетах.
- В качестве алгоритмов возьмем TD3+BC, CQL, BC и для каждого запустим перебор гиперпараметров по сетке, указанной в оригинальных статьях.

#### Как решить спустя пару недель, работает ли идея? Какие для этого есть критерии? Какие критерии того, что идея все-таки не работает?
- Главный критерий: Если графики для разных алгоритмов пересекаются, то это уже вин. 
- Критерий останова: Если не видим никаких инсайдов, то стопаем.

#### Примерная оценка по времени
- Кажется, при наличии 2-4 гпу -- все можно изи проверить за 1-2 недели. Имплементация EVP -- есть, нужно только перебрать гиперпараметры на 1-2 датасетах NeoRL (можем ограничиться 20-ой гиперами).

### Experiments B

Что делать, если идея все-таки заработает?

- Будем отталкиваться от получившихся инсайдов. В целом, можно начинать писать статью на воркшоп и выстраивать нарратив + к нему доп эксперименты.
- Из главного: больше датасетов (взять весь NeoRL), больше алгоритмов (не только 3 изначальных)
- Из интересного: сделать анализ под разными OPE методами, вдруг там что-то вылезет
